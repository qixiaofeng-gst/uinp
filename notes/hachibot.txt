2020-4-28 会：
1. 电机驱动器的问题，芯片：温度传感器、位置传感器、电流回馈，有损去噪，是用 raw 数据作为 RL 输入还是用去噪过的？
   张钰：不用，没有差异，刘博士赞同
2. 八字腿是 reward 设计的问题。
3. 输入维度增加可能有用，但目前用处未知。

warning: ‘hachi::PyWrapperingMotionController’
 declared with greater visibility than the type of its field
  ‘hachi::PyWrapperingMotionController::controllerInPy’
see: https://pybind11.readthedocs.io/en/stable/faq.html


------- -------
CoreDump info
@dacong02:~/hachidog/build# coredumpctl gdb 2542
           PID: 2542 (mit_ctrl)
           UID: 0 (root)
           GID: 0 (root)
        Signal: 6 (ABRT)
     Timestamp: 二 2020-04-28 10:47:53 CST (1min 42s ago)
  Command Line: ./mit_ctrl m r f
    Executable: /home/user/hachidog/build/mit_ctrl
 Control Group: /user.slice/user-1000.slice/session-1.scope
          Unit: session-1.scope
         Slice: user-1000.slice
       Session: 1
     Owner UID: 1000 (user)
       Boot ID: 0c77f1d1fe8a42119333f23e907c8acd
    Machine ID: 4a4b634d57307743796465444652752f
      Hostname: dacong02
      Coredump: /var/lib/systemd/coredump/core.mit_ctrl.0.0c77f1d1fe8a42119333f23e907c8acd.2542.1588042073000000000000.xz
       Message: Process 2542 (mit_ctrl) of user 0 dumped core.

                Stack trace of thread 2542:
                #0  0x00007f0c85405428 __GI_raise (libc.so.6)
                #1  0x00007f0c8540702a __GI_abort (libc.so.6)
                #2  0x00007f0c854477ea __libc_message (libc.so.6)
                #3  0x00007f0c8545213e malloc_printerr (libc.so.6)
                #4  0x00007f0c85454184 __GI___libc_malloc (libc.so.6)
                #5  0x00007f0c85402d6a _nl_normalize_codeset (libc.so.6)
                #6  0x00007f0c853fc70c _nl_load_locale_from_archive (libc.so.6)
                #7  0x00007f0c853fbafa _nl_find_locale (libc.so.6)
                #8  0x00007f0c853fafb4 __GI_setlocale (libc.so.6)
                #9  0x00007f0c87841ed4 _Py_InitializeEx_Private (libpython3.6m.so.1.0)
                #10 0x000000000053ff6b _ZN8pybind1122initialize_interpreterEb (mit_ctrl)
                #11 0x00000000005240ed _ZN5hachi28PyWrapperingMotionControllerC2ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES8_j (mit_ctrl)
                #12 0x0000000000424cd3 _ZN18FSM_State_AIMotionIfEC2EP14ControlFSMDataIfE (mit_ctrl)
                #13 0x000000000042b39c _ZN10ControlFSMIfEC2EP9QuadrupedIfEP23StateEstimatorContainerIfEP13LegControllerIfEP13GaitSchedulerIfEP19DesiredStateCommandIfEP22RobotControlParametersP17VisualizationDataP18MIT_UserParameters (mit_ctrl)
                #14 0x00000000004141e4 _ZN14MIT_Controller20initializeControllerEv (mit_ctrl)
                #15 0x00007f0c87f0b396 _ZN11RobotRunner4initEv (/home/user/hachidog/build/librobot.so)
                #16 0x00007f0c8689be69 _ZN12PeriodicTask5startEv (/home/user/hachidog/build/libbiomimetics.so)
                #17 0x00007f0c87f182c0 _ZN25MiniCheetahHardwareBridge3runEv (/home/user/hachidog/build/librobot.so)
                #18 0x00007f0c87f224e5 _Z11main_helperiPPcP15RobotController (/home/user/hachidog/build/librobot.so)
                #19 0x000000000041234e main (mit_ctrl)
                #20 0x00007f0c853f0830 __libc_start_main (libc.so.6)
                #21 0x00000000004130e9 _start (mit_ctrl)
https://bugzilla.redhat.com/show_bug.cgi?id=239344

连接 mini-cheetah，接网线，PC 关 wifi，手动配置 IP 为 10.0.0.2，子网：255.255.255.0，DNS：10.0.0.1。
然后 ssh user@10.0.0.36，密码 i-b。`cd hachidog/build` 然后 `/bin/bash ./run_mc.sh ./mit_ctrl`。

1. 刘博士提议：数值可视化监控的工具，主要针对狗狗的状态（关节位置、扭矩等）数值。

last-week:
a. AI Controller 对接模拟器排除 bug。
b. tensorflow 2.1.0 升级尝试。需要移植 PPO，目前 stable_baselines 不支持。
c. 读了一些 RL 入门的在线书籍，我们需不需要 wiki 里加一个缩略语表。

_______ _______
Deep learning:
  Function: fit, approximate
  Components: nerual network
  Applications: DNN, CNN, RNN, LSTM, GRU
Reinforcement learning:
  Function: solves sequential decision making problems
  Components: function approximator(
    shallow ones: linear function, decision trees, tile coding.
    deep ones: nerual network
  )
  Applications: Q-Learning, DQN，PPO

Deep reinforcement learning (DRL) methods use
  deep neural networks to
    approximate any of the following component of reinforcement learning:
      value function, V(s; θ) or Q(s, a; θ),
      policy π(a|s; θ) and
      model (state transition and reward).
Here, the parameters θ are the weights in deep neural networks.

RL parlances:
  Online learning, use data once then drop.
  Offline learning, use data repeatly.

  On-policy, one policy involved.
  Off-policy, at least two policy involed, one learn from another one in each step.

  Prediction problem, estimate the value of certain state or <state, action>.
  Control problem, environment is given, find the best policy.


DQN: Deep Q Network
Policy gradient methods attempt to learn funcitons with directly map an observation to an action.
Q-Learning attempts to learn the value of being in a given state and taking a specific action here.

Supervised learning has a target label for each training example.
Unsupervised learning has no labels at all.
Reinforcement learning has sparse and time-delayed labels – the rewards.

Credit assignment problem – Which of the preceding actions were responsible for getting the reward and to what extent.
Explore-exploit dilemma – Should you exploit the known working strategy or explore other, possibly better strategies.

In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers.

Markov decision process M = {S, A, T, r}:
  S - state space, s belongs to S
  A - action space, a belongs to A
  T - trainsition operator
  r - reward function, r: S x A -> R, r(s_t, a_t) - reward
Partially observed Markov decision process M = {S, A, O, T, epsilon, r}
  O - observation space, o belongs to O
  epsilon - emission probability p(o_t|s_t)
Step described by <s, a, r, s'>, s' is defined by T(s, a), r is given by R(s, a)
<s, a, r, s'> = <state, action, reward, next state>
Policy \pi(a_t|s_t) is:
  Agent behavior.
  A mapping from s_t to a_t
    receives r_t and
    transitions to s_{t+1} according to
      environment dynamics(or model),
      reward function R(s, a) and
      state transition probability P(s_{t+1}|s_t, a_t)
Value function is a prediction of the expected accumulative discounted future reward.
Value function measures how good is each state, or state-action pair.

_______ _______
CNN introduction:
Four steps in a training iteration: forward pass, loss function, backward pass, weight update
Convolution layer extract specific characteristics, which is expressed by filters, out of before layer
Padding (zeros around before layer) helps to reserve the size of the layer after applied filter
Choosing hyperparameter(filter size, stride, padding) largely depend on the type of data that you have
Nonlinear layers introduces nonlinearity, e.g. f(x) = max(0, x) which is ReLU (rectified linear units), tanh, sigmoid
Pooling layers(or downsampling layer), mainly three types: max pooling, average pooling and L2-norm pooling
# composite with a filter with stride that is same as its size, and a function, which is max/average/L2-norm implied, for generating output
# reduced afterward computation complexity
# control overfitting, which might lead to 100% fitting on training set but 50% or less on test/validation set
Dropout layers drops out a random activations set by setting them to zero
* also introduced to alleviate overfitting problem
* only used during training, not in test/validation
Network in Network Layers refers to conv layer with 1x1xN filters
Objects for networks: classification, localization, detection and segmentation
Transfer learning is the process of taking a pre-trained model (
    the weights and parameters of a network that
    has been trained on a large dataset by somebody else
  ) and “fine-tuning” the model with your own dataset.
  The idea is that this pre-trained model will act as a feature extractor.
  You will remove the last layer of the network and replace it with your own classifier (
    depending on what your problem space is
  ).
Data augmentation techniques are a way to artificially expand your dataset.
  Some popular augmentations are grayscales, horizontal flips, vertical flips,
  random crops, color jitters, translations, rotations, and much more.

_______ _______
DL introduction:
Below 'x_' stands for inputs (without suffix means vector),
      'w_' stands for weight (without suffix means vector),
      'b' stands for bias(= negative threshold, vector),
      'a' stands for output (vector) and
      'y(x)' stands for desired output (vector).
A core factor: sigma = sum_j(w_j * x_j + b);
perceptron, a = ( sigma <= 0 ) ? 0 : 1;
sigmoid, a = 1 / ( 1 + exp(-sigma) );
quadratic cost/loss/objective function, C(w, b) = sum_x(|| y(x) - a ||)^2 / 2n;
       n is the total number of training input;
       ||v|| denotes usual length function for v.
gradient descent algorithm

_______ _______
什么是数，什么是量
量有单位，数没有。数通常是一个物理量的比值。

_______ _______
Planning VS Policy

GSAO:
  G: goal
  S: state
  A: action
  O: observation

1 配置文件切换模式，硬件时遥控器切换模式。
  AI 这边的可执行内容在 so 文件里。
2 调试时用 rosbag 录制的所有消息。
  CMake 都保证存在冲突的库依赖放到相互独立的地方。

dot product (output number) VS cross product (output vector)
PD(proportional-derivative) controller: TeX[[ f = k_p (x_{ref}-x) + k_d (v_{ref}-v), k_p = \omega_n^2, k_d = 2 \xi \omega_n ]]

_______ _______
As of 2016, AlphaGo's algorithm uses a combination of machine learning and tree search techniques,
  combined with extensive training, both from human and computer play.
  It uses Monte Carlo tree search, guided by a "value network" and a "policy network," both implemented using deep neural network technology.
The application of Monte Carlo tree search in games is based on many playouts also called roll-outs.
  In each playout, the game is played out to the very end by selecting moves at random.
  The final game result of each playout is then used to weight the nodes in the game tree
  so that better nodes are more likely to be chosen in future playouts.
Each round of Monte Carlo tree search consists of four steps:
  Selection: start from root R and select successive child nodes until a leaf node L is reached.
    The root is the current game state and a leaf is any node from which no simulation (playout) has yet been initiated.
    The section below says more about a way of biasing choice of child nodes that
    lets the game tree expand towards the most promising moves,
    which is the essence of Monte Carlo tree search.
  Expansion: unless L ends the game decisively (e.g. win/loss/draw) for either player,
    create one (or more) child nodes and choose node C from one of them.
    Child nodes are any valid moves from the game position defined by L.
  Simulation: complete one random playout from node C.
    This step is sometimes also called playout or rollout.
    A playout may be as simple as choosing uniform random moves until
    the game is decided (for example in chess, the game is won, lost, or drawn).
  Backpropagation: use the result of the playout to update information in the nodes on the path from C to R.

_______ _______
Turing machine M = <A, S, R, I, F, b, s0>.
  A is alphabet symbols.
  S is states set.
  R is rules table.
  I is input symbols, subset of A.
  F is final states, subset of S.
  b is the blank symbol.
  s0 is the initial state.

Useful information:
1. pybind11 is the bridge between c++ and python
2. check Qt installation: qtdiag
3. URDF: Unified Robot Description Format; SDF: Simulation Description Format; MJCF: Multi-Joint dynamics with Contact Format
4. Eigen: C++ template library for matrix/verctor/numerical solver, pure header files without binary library
5. Executables for Qt might start correctly with -qt=qt5 as argument

这两天对需求和代码粗浅的理解之后，大概弄了个分层设计（没什么高深的，分个层方便描述、理解和沟通，预期每一层都至少是一个独立工程）：
1. 描述格式适配（输入层）
   - 输入：URDF，SDF，MJCF 等格式
   - 输出：单一格式的机器人描述，比如 URDF，或者我们自定
   - 对不同目标（模拟器或机器人）可能要进行数值调整，因此输出可能针对不同目标生成独立的版本
   - 可能是个工具集合，针对不同格式和目标有不同工具
2. 机器人控制模型训练（训练层，核心工程）
   - 输入：机器人描述，环境描述，训练过的控制模型
   - 输出：训练过的控制模型
   - 预期是 C++ 和 Python 混合的工程
   - 与输入层类似，可能针对不同目标需要不同的训练环境
   - 本层的输入输出在概念上是有不同目标之分的，但是代码逻辑上是不用区分不同目标的
   - 本层的输入输出的内容格式和训练逻辑是稳定的、一致的
   - 模型的训练预期是要依赖某种模拟器的，最好能选一个开源的模拟器
3. 机器人控制模型导出（转译/输出层）
   - 输入：训练过的控制模型
   - 输出：用于控制机器人或模拟器的可执行程序
   - 主要需要将训练过的控制模型针对不同目标进行处理
   - 可能是个工具集合，针对不同目标有不同工具
